---
title: Advanced Topics in Machine Learning
subtitle: Sheet 2
author: Submitted by - Ranji Raj
date: "April 19th, 2021"
output:
  pdf_document: default
  html_document:
    df_print: paged
  word_document: default
header-includes:
- \usepackage{titling}
- \posttitle{\end{center}} \usepackage{fancyhdr} \pagestyle{fancy} 
---
# **Assignment 2.2 - Filter Techniques**
Dataset: _Excess alcohol consumption among students_[^1]

```{r echo = FALSE, message=FALSE, warning=FALSE}
#Dataset: Alcohol Consumption
library(tidyverse)
student <- read_csv("student_alc.csv")
student <- student %>%
  map_if(is.character, as.factor) %>%
  bind_cols()
student <- student %>%
  mutate(alc_prob = ifelse(Dalc + Walc >= 6, "alc_p", "no_alc_p"))

str(student)

```
\newpage
Class distribution
```{r echo = FALSE, message=FALSE, warning=FALSE}
table(student$alc_prob)
```
__Feature Importance by Chi-squared filter__
```{r echo = FALSE, message=FALSE, warning=FALSE}
library(FSelector)
weights<- chi.squared(alc_prob~., student)

# Print the results 
print(weights)
```


```{r echo = FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(forcats)
library(stringr)
library(purrr)
library(caret)

# Function that calculates the Gini Index of a partitioning of x w.r.t. y
myGini <- function(x,y) {
  ti <- tibble(x, y) # generates a table from one attribute x (e.g. sex), and alco_prob
  rat <- prop.table(table(ti$x)) # calculates the percentage amount of females and males
  ti <- ti %>%
    split(.$x) %>% # number of males and females w.r.t alco_prob (alc and no_alc)
    map(~prop.table(table(.$y))) %>% # applies function to calculate the percentage
    #amount of alc_prop and and n_alc_prop with females ind males
    map(~ 1 - sum(.^2)) %>%
    unlist()
  return(sum(ti*rat))
}
```


```{r echo = FALSE, message=FALSE, warning=FALSE}
gini_class <- 1 - sum(prop.table(table(student$alc_prob))^2)


li_gini <- vector("list", length = ncol(student))
for(var in 1:ncol(student)){
  if(is.factor(student[[var]])) {
    df_gini <- tibble(
      variable = names(student)[[var]],
      gini = NA
    )
    df_gini$gini[1] <- myGini(student[[var]], student$alc_prob)
    li_gini[[var]] <- df_gini
  }
  # For numeric variables calculate Gini index for all possible split points
  if(is.numeric(student[[var]])) {
    split_points <- sort(unique(student[[var]]))
    df_gini <- tibble(
      variable = str_c(names(student)[[var]], "<=", split_points),
      gini = NA
    )
    for(sp in 1:length(split_points)) {
      temp_var <- cut(student[[var]], breaks = c(-Inf, split_points[sp], Inf))
      df_gini$gini[sp] <- myGini(temp_var, student$alc_prob)
    }
    
    #Choose best split, i.e. split with lowest Gini Index
    li_gini[[var]] <- df_gini %>% filter(!is.nan(gini)) %>% arrange(gini) %>% slice(1)
  }
}
```


```{r echo=FALSE, message=FALSE, warning=FALSE}
set.seed(123)
inTrain <- sample(c(FALSE, TRUE), size = nrow(student), replace = TRUE, prob = c(.3, .7))
student <- map_df(student, ~if(is.character(.)){factor(.)}else{.})
student_train <- student %>% filter(inTrain)  #Training set
student_test <- student %>% filter(!inTrain)  #Test set
#str(student_test)
```

\newpage
__Top 5 features__
```{r echo = FALSE, message=FALSE, warning=FALSE}
# Select top five variables
subset<- cutoff.k(weights, 5)
subset
```

```{r echo = FALSE, message=FALSE, warning=FALSE, out.width = '70%', fig.align="center"}
library(randomForest)
#--------------------------------------
set.seed(123)
rf <- randomForest(alc_prob ~ Dalc + Walc + goout + sex + studytime, data = student, ntree = 200, mtry = 5)
cm <- rf$confusion[1:2,1:2]
acc <- sum(diag(cm))/sum(sum(cm))
#acc
sens <- cm[1,1]/sum(cm[1,])
#sens
spec <- cm[2,2]/sum(cm[2,])
#spec

cat('Accuracy:   ', acc, '\n',
    'Sensitivity: ', sens, '\n',
    'Specificity: ', spec, '\n',sep='')
```

```{r echo = FALSE, message=FALSE, warning=FALSE, out.width = '70%', fig.align="center"}
varImpPlot(rf, type=2, main = 'Variable Importance of top 5')
```
```{r}
```
__Observation__: The plot is indicative that the feature `Walc` which is to the extreme right is of very high importance relative to other features. The value is nearly `80` very much emphasizes that the volume of subset of features is crucial to the classification power of the model.

__Top 10 features__
```{r echo = FALSE, message=FALSE, warning=FALSE, out.width = '70%', fig.align="center"}
# Select top five variables
subset<- cutoff.k(weights, 10)
subset
```

```{r echo = FALSE, message=FALSE, warning=FALSE, out.width = '70%', fig.align="center"}
library(randomForest)
#--------------------------------------
set.seed(123)
rf <- randomForest(alc_prob ~ Walc + Dalc + goout + sex + studytime + reason + famsup + higher + famsize + Fjob , data = student, ntree = 200, mtry = 5)
cm <- rf$confusion[1:2,1:2]
acc <- sum(diag(cm))/sum(sum(cm))
#acc
sens <- cm[1,1]/sum(cm[1,])
#sens
spec <- cm[2,2]/sum(cm[2,])
#spec

cat('Accuracy:   ', acc, '\n',
    'Sensitivity: ', sens, '\n',
    'Specificity: ', spec, '\n',sep='')
```

```{r echo = FALSE, message=FALSE, warning=FALSE, out.width = '70%', fig.align="center"}
varImpPlot(rf, type=2, main = 'Variable Importance of top 10')
```
__Observation__: The plot is indicative that the scale of `MeanDecreaseGini` is adapted according to the features taken a hand. Similar as to the former plot the feature `Walc` which is to the extreme right is of very high importance followed by `Dalc` relative to other features.

__Top 15 features__
```{r echo = FALSE, message=FALSE, warning=FALSE, out.width = '70%', fig.align="center"}
# Select top five variables
subset<- cutoff.k(weights, 15)
subset
```


```{r echo = FALSE, message=FALSE, warning=FALSE, out.width = '70%', fig.align="center"}
library(randomForest)
#--------------------------------------
set.seed(123)
rf <- randomForest(alc_prob ~ Walc + Dalc + goout + sex + studytime + reason + famsup + higher + famsize + Fjob + nursery + Mjob + romantic + schoolsup + activities , data = student, ntree = 200, mtry = 5)
cm <- rf$confusion[1:2,1:2]
acc <- sum(diag(cm))/sum(sum(cm))
#acc
sens <- cm[1,1]/sum(cm[1,])
#sens
spec <- cm[2,2]/sum(cm[2,])
#spec

cat('Accuracy:   ', acc, '\n',
    'Sensitivity: ', sens, '\n',
    'Specificity: ', spec, '\n',sep='')
```

```{r echo = FALSE, message=FALSE, warning=FALSE, out.width = '70%', fig.align="center"}
varImpPlot(rf, type=2, main = 'Variable Importance of top 15')
```
__Observation__: The plot is indicative that the scale of `MeanDecreaseGini` is adapted according to the features taken a hand. Similar as to the former plot the feature `Walc` which is to the extreme right is of very high importance but at the same time it has an impact by the other 14 variables relative to other features.

__Top 20 features__
```{r echo = FALSE, message=FALSE, warning=FALSE, out.width = '70%', fig.align="center"}
# Select top five variables
subset<- cutoff.k(weights, 20)
subset
```

```{r echo = FALSE, message=FALSE, warning=FALSE}
library(randomForest)
#--------------------------------------
set.seed(123)
rf <- randomForest(alc_prob ~ Walc + Dalc + goout + sex + studytime + reason + famsup + higher + famsize + Fjob + nursery + Mjob + romantic + schoolsup + activities + Pstatus + guardian + internet + paid + age , data = student, ntree = 200, mtry = 5)
cm <- rf$confusion[1:2,1:2]
acc <- sum(diag(cm))/sum(sum(cm))
#acc
sens <- cm[1,1]/sum(cm[1,])
#sens
spec <- cm[2,2]/sum(cm[2,])
#spec

cat('Accuracy:   ', acc, '\n',
    'Sensitivity: ', sens, '\n',
    'Specificity: ', spec, '\n',sep='')
```

```{r echo = FALSE, message=FALSE, warning=FALSE, out.width = '70%', fig.align="center"}
varImpPlot(rf, type=2, main = 'Variable Importance of top 20')
```
__Observation__: Similar as to the former plot the feature `Walc` which is to the extreme right is now with a value of `50` relative to other features.

[^1]: https://www.kaggle.com/uciml/student-alcohol-consumption
