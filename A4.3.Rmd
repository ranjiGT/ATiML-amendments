---
title: Advanced Topics in Machine Learning
subtitle: Sheet 4
author: Submitted by - Ranji Raj
date: "May 03, 2021"
output:
  pdf_document: default
  word_document: default
  html_document:
    df_print: paged
header-includes:
- \usepackage{titling}
- \posttitle{\end{center}} \usepackage{fancyhdr} \pagestyle{fancy} 
---
# **Assignment 4.3 - Model comparison using Learning curves**

__TASK A__

_Hyperparameters_

**Logistic regression**      | **Naive Bayes**      
---------------| -----------------
__Solver__ (small datasets, multi-class problems)  | __Smoothing parameter__ (alpha)
__Penalty__(`L1`, `L2`, `elasticnet`)        | ( _For handling zero-frequency problem_ )
__class_weightdict or ‘balanced’__ (for imbalanced data)  | 
__multi_class__ |  
|

__TASK B__
```{r echo=FALSE, message=FALSE, warning=FALSE, out.width = '70%', fig.align="center"}

library(data.table, warn.conflicts=F)
library(ggplot2,warn.conflicts=F)
library(plotly,warn.conflicts=F)

library(tidyverse,warn.conflicts=F)

library(forcats,warn.conflicts=F)
library(stringr,warn.conflicts=F)
library(caTools,warn.conflicts=F)

library(caret,warn.conflicts=F)
require(reshape2,warn.conflicts=F)

library(corrplot,warn.conflicts=F)
library(factoextra,warn.conflicts=F)
library(gridExtra,warn.conflicts=F)

# library(highcharter,warn.conflicts=F)
library(rpart,warn.conflicts=F)
library(e1071,warn.conflicts=F)
library(ranger,warn.conflicts=F)
#library(epiR,warn.conflicts=F)
library(randomForest,warn.conflicts=F)
#library(party,warn.conflicts=F)
library(class,warn.conflicts=F)
library(kknn,warn.conflicts=F) 
# library(gbm,warn.conflicts=F)
# library(ada,warn.conflicts=F)
```

Dataset: _Breast-cancer Wisconsin-data_[^1]
```{r echo=FALSE, message=FALSE, warning=FALSE, out.width = '70%', fig.align="center"}

data <- read.csv("data.csv", header=T)
glimpse(data)
data<-data[,-33]
```
[^1]: https://www.kaggle.com/uciml/breast-cancer-wisconsin-data

```{r echo=FALSE, message=FALSE, warning=FALSE,results='hide',out.width = '70%', fig.align="center", fig.show='hide'}
missing_values <- data %>% summarize_all(funs(sum(is.na(.))/n()))

missing_values <- gather(missing_values, key="feature", value="missing_pct")

missing_values %>% 

  ggplot(aes(x=reorder(feature,-missing_pct),y=missing_pct)) +

  geom_bar(stat="identity",fill="red")+

  coord_flip()+theme_bw()
```
```{r echo=FALSE, message=FALSE, warning=FALSE, results='hide',out.width = '70%', fig.align="center"}

table(data$diagnosis)
prop.table(table(data$diagnosis))*100
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='hide', out.width = '70%', fig.align="center"}
data$diagnosis<-factor(data$diagnosis, labels=c('B','M'))
prop.table(table(data$diagnosis))*100
dataset<-data
#head(dataset)
```


___Class distribution across training and test set___
```{r echo=FALSE, message=FALSE, warning=FALSE, out.width = '70%', fig.align="center"}

set.seed(123)
smp_size <- floor(0.67 * nrow(dataset))
train_ind <- sample(seq_len(nrow(dataset)), size = smp_size)
train <- dataset[train_ind, ]
test <- dataset[-train_ind, ]
cat("Training set ")
prop.table(table(train$diagnosis))*100
cat("Test set ")
prop.table(table(test$diagnosis))*100
```

```{r echo=FALSE, message=FALSE, warning=FALSE, out.width = '70%', fig.align="center", results='hide'}
fitControl <- trainControl(
  method = "cv",
  number = 10,
  savePredictions = TRUE)

lreg<-train(diagnosis~.,data=train[,-1],method="glm",family=binomial(),
             trControl=fitControl)
#varImp(lreg)
```


```{r echo=FALSE, message=FALSE, warning=FALSE, out.width = '70%', fig.align="center", results='hide'}
lreg_pred <- predict(lreg,test[,-c(1,2)])
cm_logistic <- confusionMatrix(lreg_pred,test$diagnosis)
cm_logistic
```

```{r echo=FALSE, message=FALSE, warning=FALSE, out.width = '70%', fig.align="center", results='hide'}
learn_nb <- naiveBayes(train[,-c(1,2)], train$diagnosis)

pre_nb <- predict(learn_nb, test[,-c(1,2)])
cm_nb <- confusionMatrix(pre_nb, test$diagnosis)
cm_nb
```

```{r echo=FALSE, message=FALSE, warning=FALSE, out.width = '65%', fig.align="center", fig.cap="Confusion Matrix"}

col <- c("#ed3b3b", "#0099ff")
par(mfrow=c(1,2))
fourfoldplot(cm_logistic$table, color = col, conf.level = 0, margin = 1, main=paste("Logistic (",round(cm_logistic$overall[1]*100),"%)",sep=""))

fourfoldplot(cm_nb$table, color = col, conf.level = 0, margin = 1, main=paste("NaiveBayes (",round(cm_nb$overall[1]*100),"%)",sep=""))

```

```{r echo=FALSE, message=FALSE, warning=FALSE, out.width = '90%', fig.align="center", fig.cap="Learning curves for Wisconsin Breast-Cancer data"}

library(mlr)
library(e1071)
library(LiblineaR)
lrns = list("classif.naiveBayes", "classif.LiblineaRL2LogReg")
rin = makeResampleDesc(method = "CV", iters = 5)
lc = generateLearningCurveData(learners = lrns, task=makeClassifTask(data = train, target = "diagnosis"),
  percs = seq(0.2, 1, by = 0.2), measures=ber,
  resampling = rin, show.info = FALSE)
plotLearningCurve(lc, pretty.names = T)

```
In comparison to the publication [^2] on the generated learning curve, it can be observed that the semantics of the curve closely resemble. For the `40%` of the training data the error is low for `Naive Bayes`, Even with L2 regularization on logistic regression the error rate asymptotically touches to `0.5`.

[^2]: http://www.robotics.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf

__TASK C__

__Inferences from learning curve__

- For `Logisitic regression`, with L2 regularization applied, it is observed that for all the portions of the training samples (percentage) the balanced error rate is constant with `0.5`.

- For `Naive Bayes`, the error rates for `r nrow(train)*0.2`, `r nrow(train)*0.4`, `r nrow(train)*0.6`. `r nrow(train)*0.8` and `r nrow(train)` proportions respectively of the data is below `0.1`.

- From a diagnostic perspective, it can be said that,
  - For `logisitic regression` the training error remains flat regardless of training which is indicative of `underfitting`.
  - For `Naive bayes` the training error continues to decrease until the end of training which is also indicative of`underfitting`.

