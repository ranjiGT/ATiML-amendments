---
title: Advanced Topics in Machine Learning
subtitle: Sheet 2
author: Submitted by - Ranji Raj
date: "April 18th, 2021"
output:
  pdf_document: default
  html_document:
    df_print: paged
  word_document: default
header-includes:
- \usepackage{titling}
- \posttitle{\end{center}} \usepackage{fancyhdr} \pagestyle{fancy} 
---
# **Assignment 2.1 - Filter Techniques (Feature Selection)**
__Bag of Words (BoW)__:

- One of the most used techniques to transforms text into independent variables is that called __Bag of Words (BoW)__.
- Fully understanding the text is difficult, but _BoW_ provides a very simple approach: it just counts the number of times each word appears in the text and uses these counts as the independent variables.
- In BoW, there is exactly _one feature for each word_. This is a very simple approach, but is often very effective, too. It is used as a baseline in text analytics projects and for Natural Language Processing.

```{r echo = FALSE, message=FALSE, warning=FALSE}
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
filePath <- "http://www.sthda.com/sthda/RDoc/example-files/martin-luther-king-i-have-a-dream-speech.txt"
text <- readLines(filePath)
docs <- Corpus(VectorSource(text))
#inspect(docs)

toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")

# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2")) 
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
# docs <- tm_map(docs, stemDocument)

dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)

# source('http://www.sthda.com/upload/rquery_wordcloud.r')
# filePath <- "http://www.sthda.com/sthda/RDoc/example-files/martin-luther-king-i-have-a-dream-speech.txt"
# res<-rquery.wordcloud(filePath, type ="file", lang = "english",min.freq = 1,  max.words = 30)
```
   
We observe that BOW contains features that are nominal(categorical) in nature. For Nominal features we can use:

 __Chi-squared - Filter__

Dataset: _HouseVotes84_
```{r echo = FALSE, message=FALSE, warning=FALSE}
# Use HouseVotes84 data from  mlbench package
library(mlbench)# For data
library(FSelector)#For method
data(HouseVotes84)
str(HouseVotes84)
#Calculate the chi square statistics 
weights<- chi.squared(Class~., HouseVotes84)

# Print the results 
print(weights)

# Select top five variables
subset<- cutoff.k(weights, 5)

# Print the final formula that can be used in classification
f<- as.simple.formula(subset, "Class")
print(f)
```
 __Mutual Information - Filter__

Dataset: _Iris_
```{r echo = FALSE, message=FALSE, warning=FALSE}
library(praznik)
miScores(iris[,-5],iris$Species)
```

Filter methods such as __t-test(2 classes)__ or __ANOVA(more than 2 classes)__ are _least suitable_ as they require input features to be numeric.

Filter methods such as __Pearson's product-moment correlation__ are _not suitable_ as they not only require input features to be numeric but also the target variables.