---
title: Advanced Topics in Machine Learning
subtitle: Sheet 4
author: Submitted by - Ranji Raj
date: "May 02nd, 2021"
output:
  pdf_document: default
  word_document: default
  html_document:
    df_print: paged
header-includes:
- \usepackage{titling}
- \posttitle{\end{center}} \usepackage{fancyhdr} \pagestyle{fancy} 
---
# **Assignment 4.1 - Data sampling techniques & strategies**

__Subtask a__
```{r echo = FALSE, message=FALSE, warning=FALSE, out.width = '70%', fig.align="center"}
library(mvtnorm)
library(fontawesome)
a1 <- c(1, 0)
a2 <- c(0, 1)
M <- cbind(a1, a2)

C0 <- rmvnorm(100, c(0, 0), M)
C1 <- rmvnorm(100, c(5, 0), M)

plot(C0, col = "red", xlim = c(-5, 10), ylim = c(-5, 5), xlab = "X", ylab = "Y", main = '200 samples, mean 0 and 5, S.D=0.5')
points(C1, col = "blue")
legend("topright", inset = .05, c("Class 0", "Class 1"), fill = c("red", "blue"))
```
__Observation__: Since the mean of __Class 1__ is __5.0__ the separation to other class 0 is a bit far away, by which the visualization of two classes with 200 instances can be made made properly.

\newpage
__Subtask b__
```{r echo = FALSE, message=FALSE, warning=FALSE, out.width = '70%', fig.align="center"}
a1 <- c(1, 0)
a2 <- c(0, 1)
M <- cbind(a1, a2)

C0 <- rmvnorm(100, c(0, 0), M)
C2 <- rmvnorm(100, c(2.25, 0), M)

plot(C0, col = "red", xlim = c(-5, 10), ylim = c(-5, 5), xlab = "X", ylab = "Y", main = '200 samples, mean 0 and 2.25, S.D=0.5')
points(C2, col = "green")
legend("topright", inset = .05, c("Class 0", "Class 2"), fill = c("red", "green"))
```
__Observation__: Since the mean of __Class 2__ is __2.25__ the closeness to  class 0 is nearest, by which the visualization of two classes with 200 instances is difficult to make.

__Subtask c__

Single dataset of 200 samples with _Simple random Sampling (SRS)_ ( _from subtask a_)

```{r echo = FALSE, message=FALSE, warning=FALSE, out.width = '70%', fig.align="center"}
dat <- rbind(C0, C1)
C <- data.frame(dat)
y <- sign(-1 - 2 * dat[,1] + 4 * dat[,2] )
y[y == -1] <- 0
df1 <- cbind.data.frame(y, C)
str(df1)
 
library(caret)
#Create training and test sets
set.seed(200)
trainIndex <- sample(c(FALSE,TRUE), size = nrow(df1), prob = c(.33,.67), replace = TRUE)
train_set <- df1[trainIndex, ]
test_set <- df1[!trainIndex, ]
# Learn Logistic Regression Model
fit <- glm(y ~ ., data = train_set, family = "binomial")
pred <- predict(fit, newdata = test_set, type = "response")
tab <- table(actual = test_set$y, predicted = round(pred))
cm1 <- confusionMatrix(tab)
cm1
slope <- coef(fit)[2]/(-coef(fit)[3])
intercept <- coef(fit)[1]/(-coef(fit)[3]) 

library(lattice)
xyplot( X2 ~ X1  , data = df1, groups = y,main="Decision boundary for Logisitc Regression",
   panel=function(...){
       panel.xyplot(...)
       panel.abline(intercept , slope)
       panel.grid(...)
       })
```
__Observation__: The SRS approach is indicative that the classes appear to be well-separated.

__Disproportional-Stratified Random Sampling__

__Strata 1: 120 samples (60%)__
```{r echo = FALSE, message=FALSE, warning=FALSE, out.width = '70%', fig.align="center"}
library(splitstackshape)
library(tidyverse)
strata1 <- stratified(df1, "y", 0.6)
glimpse(strata1)
```
```{r echo = FALSE, message=FALSE, warning=FALSE, out.width = '70%', fig.align="center"}
#Create training and test sets
set.seed(200)
trainIndex <- sample(c(FALSE,TRUE), size = nrow(strata1), prob = c(.33,.67), replace = TRUE)
train_set <- strata1[trainIndex, ]
test_set <- strata1[!trainIndex, ]
# Learn Logistic Regression Model
fit2 <- glm(y ~ ., data = train_set, family = "binomial")
pred <- predict(fit2, newdata = test_set, type = "response")
tab2 <- table(actual = test_set$y, predicted = round(pred))
cm2 <- confusionMatrix(tab2)
cm2
slope <- coef(fit2)[2]/(-coef(fit2)[3])
intercept <- coef(fit2)[1]/(-coef(fit2)[3]) 

library(lattice)
xyplot( X2 ~ X1  , data = strata1, groups = y,main="Disproportional Stratification on 60% of 200 samples",
   panel=function(...){
       panel.xyplot(...)
       panel.abline(intercept , slope)
       panel.grid(...)
       })
```
__Observation__: The decision boundary is indicative that after the disproportionate stratified RS with only _60% of 200_ samples the classes appear to be well-separated.


__Strata 2: 80 samples (40%)__
```{r echo = FALSE, message=FALSE, warning=FALSE, out.width = '70%', fig.align="center"}
library(splitstackshape)
library(tidyverse)
strata2 <- stratified(df1, "y", 0.4)
glimpse(strata2)
```
```{r echo = FALSE, message=FALSE, warning=FALSE, out.width = '70%', fig.align="center"}
#Create training and test sets
set.seed(200)
trainIndex <- sample(c(FALSE,TRUE), size = nrow(strata2), prob = c(.33,.67), replace = TRUE)
train_set <- strata2[trainIndex, ]
test_set <- strata2[!trainIndex, ]
# Learn Logistic Regression Model
fit3 <- glm(y ~ ., data = train_set, family = "binomial")
pred <- predict(fit3, newdata = test_set, type = "response")
tab3 <- table(actual = test_set$y, predicted = round(pred))
cm3 <- confusionMatrix(tab3)
cm3
slope <- coef(fit3)[2]/(-coef(fit3)[3])
intercept <- coef(fit3)[1]/(-coef(fit3)[3]) 

library(lattice)
xyplot( X2 ~ X1  , data = strata2, groups = y,main="Disproportional Stratification on 40% of 200 samples",
   panel=function(...){
       panel.xyplot(...)
       panel.abline(intercept , slope)
       panel.grid(...)
       })
```

__Observation__: Even after the disproportionate stratified RS with only _40% of 200_ samples the classes appear to be well-separated.

__Subtask d__

The hypothesis space of logistic regression comprises all functions of the form:
$$
\sigma(z)=\frac{1}{1+e^{-z}}
$$
[`r fa("github", fill = "black")`](https://github.com/ranjiGT/ATiML-amendments)

```{r echo = FALSE, message=FALSE, warning=FALSE, out.width = '70%', fig.align="center"}
plot(C0, y, xlab="C0", ylab="y", main = "Hypothesis space of C0 vs. y") 
g = glm(y~X1, family=binomial, df1) 
curve(predict(g,data.frame(X1=x),type="resp"),add=TRUE) 
```
```{r echo = FALSE, message=FALSE, warning=FALSE, out.width = '70%', fig.align="center"}
plot(C1, y, xlab="C1", ylab="y", main = "Hypothesis space of C1 vs. y") 
g = glm(y~X2, family=binomial, df1) 
curve(predict(g,data.frame(X2=x),type="resp"),add=TRUE) 
```
__Subtask e__

The true target function ( _from task c_) can be formulated as:

For SRS:

$$
\sigma(y)=\frac{1}{1+e^{-(-35.76-74.24X1)}},
\sigma(y)=\frac{1}{1+e^{-(-35.76+146.30X2)}}
$$
For Strata 1:
$$
\sigma(y)=\frac{1}{1+e^{-(-25.03-59.61X1)}},
\sigma(y)=\frac{1}{1+e^{-(-25.03+117.25X2)}}
$$
For Strata 2:

$$
\sigma(y)=\frac{1}{1+e^{-(-35.76-74.24X1)}},
\sigma(y)=\frac{1}{1+e^{-(-35.76+146.30X2)}}
$$